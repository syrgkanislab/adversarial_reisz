{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Clone GitHub repo\n",
        "!git clone https://github.com/vsyrgkanis/adversarial_reisz.git\n",
        "%cd adversarial_reisz\n",
        "# !git checkout charitable-giving"
      ],
      "metadata": {
        "id": "LJJePSJpnOsc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c792d192-bee3-4a78-f041-f47fb7a2ea20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'adversarial_reisz'...\n",
            "remote: Enumerating objects: 479, done.\u001b[K\n",
            "remote: Counting objects: 100% (166/166), done.\u001b[K\n",
            "remote: Compressing objects: 100% (106/106), done.\u001b[K\n",
            "remote: Total 479 (delta 87), reused 121 (delta 60), pack-reused 313\u001b[K\n",
            "Receiving objects: 100% (479/479), 9.95 MiB | 25.78 MiB/s, done.\n",
            "Resolving deltas: 100% (248/248), done.\n",
            "/content/adversarial_reisz\n",
            "Branch 'charitable-giving' set up to track remote branch 'charitable-giving' from 'origin'.\n",
            "Switched to a new branch 'charitable-giving'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzINppH4JUcl"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install econml"
      ],
      "metadata": {
        "id": "RuJJFWntKBLA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39e98847-0717-4ed9-bec5-853ff5225185"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting econml\n",
            "  Downloading econml-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from econml) (1.23.5)\n",
            "Requirement already satisfied: scipy>1.4.0 in /usr/local/lib/python3.10/dist-packages (from econml) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn<1.3,>0.22.0 in /usr/local/lib/python3.10/dist-packages (from econml) (1.2.2)\n",
            "Collecting sparse (from econml)\n",
            "  Downloading sparse-0.14.0-py2.py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.0/81.0 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from econml) (1.3.2)\n",
            "Requirement already satisfied: statsmodels>=0.10 in /usr/local/lib/python3.10/dist-packages (from econml) (0.14.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from econml) (1.5.3)\n",
            "Collecting shap<0.42.0,>=0.38.1 (from econml)\n",
            "  Downloading shap-0.41.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (572 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m572.6/572.6 kB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lightgbm in /usr/local/lib/python3.10/dist-packages (from econml) (4.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.3,>0.22.0->econml) (3.2.0)\n",
            "Requirement already satisfied: tqdm>4.25.0 in /usr/local/lib/python3.10/dist-packages (from shap<0.42.0,>=0.38.1->econml) (4.66.1)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.10/dist-packages (from shap<0.42.0,>=0.38.1->econml) (23.2)\n",
            "Collecting slicer==0.0.7 (from shap<0.42.0,>=0.38.1->econml)\n",
            "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap<0.42.0,>=0.38.1->econml) (0.58.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap<0.42.0,>=0.38.1->econml) (2.2.1)\n",
            "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.10->econml) (0.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->econml) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->econml) (2023.3.post1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap<0.42.0,>=0.38.1->econml) (0.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.2->statsmodels>=0.10->econml) (1.16.0)\n",
            "Installing collected packages: slicer, sparse, shap, econml\n",
            "Successfully installed econml-0.14.1 shap-0.41.0 slicer-0.0.7 sparse-0.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python setup.py develop"
      ],
      "metadata": {
        "id": "4zO5W2-WJ6dU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c75e0b8e-213f-4dfa-f5c3-2d063864fbd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running develop\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/command/develop.py:40: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer, pypa/build or\n",
            "        other standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  easy_install.initialize_options(self)\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer, pypa/build or\n",
            "        other standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running egg_info\n",
            "creating advreisz.egg-info\n",
            "writing advreisz.egg-info/PKG-INFO\n",
            "writing dependency_links to advreisz.egg-info/dependency_links.txt\n",
            "writing top-level names to advreisz.egg-info/top_level.txt\n",
            "writing manifest file 'advreisz.egg-info/SOURCES.txt'\n",
            "reading manifest file 'advreisz.egg-info/SOURCES.txt'\n",
            "writing manifest file 'advreisz.egg-info/SOURCES.txt'\n",
            "running build_ext\n",
            "Creating /usr/local/lib/python3.10/dist-packages/advreisz.egg-link (link to .)\n",
            "Adding advreisz 0.0.1 to easy-install.pth file\n",
            "\n",
            "Installed /content\n",
            "Processing dependencies for advreisz==0.0.1\n",
            "Finished processing dependencies for advreisz==0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jsgw7d2oJUcm",
        "outputId": "de89f18f-87f4-4daa-fe97-11e287595114",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: True\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import joblib\n",
        "from google.colab import files\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import scipy\n",
        "import scipy.special\n",
        "from sklearn.linear_model import LassoCV, LogisticRegressionCV, LinearRegression, Lasso, LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.base import clone\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.utils.multiclass import type_of_target\n",
        "\n",
        "import statsmodels.api as sm  # For probit\n",
        "from utilities import mean_ci\n",
        "import scipy.stats as sps\n",
        "import statistics\n",
        "\n",
        "from debiased import DebiasedMoment\n",
        "from advreisz.linear import SparseLinearAdvRiesz\n",
        "from advreisz.kernel import AdvNystromKernelReisz, AdvKernelReisz, NystromKernelReisz, KernelReisz\n",
        "from advreisz.deepreisz import AdvReisz\n",
        "from advreisz.ensemble import AdvEnsembleReisz, AdvEnsembleReiszRegVariant, RFrr, interactive_poly_feature_fns\n",
        "from utilities import AutoKernel, prod_kernel, PluginRR, PluginRR2, FitParamsWrapper\n",
        "\n",
        "from experiments import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "O2KC8DW4cecg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regression Estimator"
      ],
      "metadata": {
        "id": "sm9WFZk-wYTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_reg_fn(X, y):\n",
        "    est = Pipeline([('p', PolynomialFeatures(degree=2, include_bias=False)),\n",
        "                    ('s', StandardScaler()),\n",
        "                    ('lasso', LassoCV(max_iter=10000, random_state=123))])\n",
        "    est.fit(X, y)\n",
        "\n",
        "    return lambda: Pipeline([('p', PolynomialFeatures(degree=2, include_bias=False)),\n",
        "                             ('s', StandardScaler()),\n",
        "                             ('lasso', Lasso(alpha=est.named_steps['lasso'].alpha_, max_iter=10000, random_state=123))])"
      ],
      "metadata": {
        "id": "0Fj5S-4kwV91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXUcNKN0JUco"
      },
      "source": [
        "# Riesz Estimators (Not Dependent On #Treatments)\n",
        "*The plugin logistic, plugin RF, and adversarial RF estimators depend on the\n",
        "number of \"treatments\", the variables that are counterfactually varied. All other Riesz estimators do not. See the moment function with two treatment for more details.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhS4TxRyJUco",
        "outputId": "23deb435-392b-4f4e-dfb6-b7bb5deaa93a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: True\n"
          ]
        }
      ],
      "source": [
        "def get_splin_fn(X):\n",
        "    return lambda: SparseLinearAdvRiesz(moment_fn,\n",
        "                                        featurizer=Pipeline([('p', PolynomialFeatures(degree=2, include_bias=False)),\n",
        "                                                             ('s', StandardScaler()),\n",
        "                                                             ('cnt', PolynomialFeatures(degree=1, include_bias=True))]),\n",
        "                                        n_iter=50000, lambda_theta=0.01, B=10,\n",
        "                                        tol=0.00001)\n",
        "\n",
        "def get_advnyskernel_fn(X):\n",
        "    n_components = 100\n",
        "    est = AdvNystromKernelReisz(kernel=lambda X, Y=None: prod_kernel(X, Y=Y, gamma=1.0/X.shape[1]),\n",
        "                                regm='auto', regl='auto', n_components=n_components, random_state=123)\n",
        "    reg = est.opt_reg(X)\n",
        "    return lambda: AdvNystromKernelReisz(kernel=lambda X, Y=None: prod_kernel(X, Y=Y, gamma=1.0/X.shape[1]),\n",
        "                                         regm=6*reg, regl=reg, n_components=n_components, random_state=123)\n",
        "\n",
        "def get_advnyskernel_fn_1000(X):\n",
        "    n_components = 1000\n",
        "    est = AdvNystromKernelReisz(kernel=lambda X, Y=None: prod_kernel(X, Y=Y, gamma=1.0/X.shape[1]),\n",
        "                                regm='auto', regl='auto', n_components=n_components, random_state=123)\n",
        "    reg = est.opt_reg(X)\n",
        "    return lambda: AdvNystromKernelReisz(kernel=lambda X, Y=None: prod_kernel(X, Y=Y, gamma=1.0/X.shape[1]),\n",
        "                                         regm=6*reg, regl=reg, n_components=n_components, random_state=123)\n",
        "\n",
        "# def get_advkernel_fn(X):  # Very computationally intensive; more practical to use a Nystrom approximation with 1000 components instead (see above)\n",
        "#     est = AdvKernelReisz(kernel=lambda X, Y=None: prod_kernel(X, Y=Y, gamma=1.0/X.shape[1]), regm='auto', regl='auto')\n",
        "#     reg = est.opt_reg(X)\n",
        "#     return lambda: AdvKernelReisz(kernel=lambda X, Y=None: prod_kernel(X, Y=Y, gamma=1.0/X.shape[1]), regm=6*reg, regl=reg)\n",
        "\n",
        "device = torch.cuda.current_device() if torch.cuda.is_available() else None\n",
        "print(\"GPU:\", torch.cuda.is_available())\n",
        "\n",
        "# Returns a deep model for the reisz representer\n",
        "def get_learner(n_t, n_hidden, p):\n",
        "    return nn.Sequential(nn.Dropout(p=p), nn.Linear(n_t, n_hidden), nn.LeakyReLU(),\n",
        "                         nn.Dropout(p=p), nn.Linear(n_hidden, n_hidden), nn.LeakyReLU(),\n",
        "                         nn.Dropout(p=p), nn.Linear(n_hidden, 1))\n",
        "\n",
        "# Returns a deep model for the test functions\n",
        "def get_adversary(n_z, n_hidden, p):\n",
        "    return nn.Sequential(nn.Dropout(p=p), nn.Linear(n_z, n_hidden), nn.ReLU(),\n",
        "                         nn.Dropout(p=p), nn.Linear(n_hidden, n_hidden), nn.ReLU(),\n",
        "                         nn.Dropout(p=p), nn.Linear(n_hidden, 1))\n",
        "\n",
        "def get_agmm_fn(X):\n",
        "    torch.manual_seed(123)\n",
        "    n_hidden = 100\n",
        "    dropout = 0.5\n",
        "    return lambda: FitParamsWrapper(AdvReisz(get_learner(X.shape[1], n_hidden, dropout),\n",
        "                                             get_adversary(X.shape[1], n_hidden, dropout),\n",
        "                                             moment_fn),\n",
        "                                   val_fr=.2,\n",
        "                                   preprocess_epochs=200,\n",
        "                                   earlystop_rounds=100,\n",
        "                                   store_test_every=20,\n",
        "                                   learner_lr=1e-4, adversary_lr=1e-4,\n",
        "                                   learner_l2=6e-4, adversary_l2=1e-4,\n",
        "                                   n_epochs=1000, bs=100,\n",
        "                                   logger=None, model_dir=str(Path.home()), device=device, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIglXV7kJUcp"
      },
      "source": [
        "# Charitable Giving Data Results\n",
        "*This section conducts analysis on the charitable giving data of \"Does Price Matter in Charitable Giving? Evidence from a Large-Scale Natural Field Experiment\" (Karlan & List, 2007).*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Moment Function (Two Treatments) – Diff-in-Diff\n",
        "*Both variables D and A are treatments here, and the moment function of interest is a classic difference-in-differences function.*"
      ],
      "metadata": {
        "id": "H4roj2khuRoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# E[E[Y|D=1, A=1, X] – E[Y|D=0, A=1, X] – (E[Y|D=1, A=0, X] – E[Y|D=0, A=0, X])]\n",
        "# D is the first column, A is the second, and X is the remaining columns.\n",
        "def moment_fn(x, test_fn):\n",
        "    n_obs = x.shape[0]\n",
        "    if torch.is_tensor(x):\n",
        "        with torch.no_grad():\n",
        "            t11 = torch.cat([torch.ones((n_obs, 2)).to(device), x[:, 2:]], dim=1)\n",
        "            t01 = torch.cat([torch.zeros((n_obs, 1)).to(device), torch.ones((n_obs, 1)).to(device), x[:, 2:]], dim=1)\n",
        "            t10 = torch.cat([torch.ones((n_obs, 1)).to(device), torch.zeros((n_obs, 1)).to(device), x[:, 2:]], dim=1)\n",
        "            t00 = torch.cat([torch.zeros((n_obs, 2)).to(device), x[:, 2:]], dim=1)\n",
        "    else:\n",
        "        t11 = np.hstack([np.ones((n_obs, 2)), x[:, 2:]])\n",
        "        t01 = np.hstack([np.zeros((n_obs, 1)), np.ones((n_obs, 1)), x[:, 2:]])\n",
        "        t10 = np.hstack([np.ones((n_obs, 1)), np.zeros((n_obs, 1)), x[:, 2:]])\n",
        "        t00 = np.hstack([np.zeros((n_obs, 2)), x[:, 2:]])\n",
        "    return test_fn(t11) - test_fn(t01) - test_fn(t10) + test_fn(t00)"
      ],
      "metadata": {
        "id": "003d9DAPt-BZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Riesz Estimators (Two Treatments)\n",
        "*The Riesz estimators for two treatments are defined here.*"
      ],
      "metadata": {
        "id": "7-1soERcuwaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lg_plugin_fn(X):\n",
        "    clf = LogisticRegressionCV(cv=3, max_iter=10000, random_state=123)\n",
        "    C_ = clf.fit(X[:, 2:], X[:, 1]).C_[0]\n",
        "    model_t_A = LogisticRegression(C=C_, max_iter=10000, random_state=123)\n",
        "    clf = LogisticRegressionCV(cv=3, max_iter=10000, random_state=123)\n",
        "    C_ = clf.fit(X[:, 1:], X[:, 0]).C_[0]\n",
        "    model_t_treat = LogisticRegression(C=C_, max_iter=10000, random_state=123)\n",
        "    return lambda: PluginRR2(model_t_A=model_t_A, model_t_treat=model_t_treat,\n",
        "                             min_propensity=1e-6)\n",
        "\n",
        "def get_rf_plugin_fn(X):\n",
        "    gcv = GridSearchCV(RandomForestClassifier(bootstrap=True, random_state=123),\n",
        "                       param_grid={'max_depth': [3, None],\n",
        "                                   'min_samples_leaf': [10, 50]},\n",
        "                       scoring='r2',\n",
        "                       cv=5)\n",
        "    best_model_A = clone(gcv.fit(X[:, 2:], X[:, 1]).best_estimator_)\n",
        "    best_model_treat = clone(clone(gcv).fit(X[:, 1:], X[:, 0]).best_estimator_)\n",
        "    return lambda: PluginRR2(model_t_A=best_model_A,\n",
        "                             model_t_treat=best_model_treat,\n",
        "                             min_propensity=1e-6)\n",
        "\n",
        "def get_rf_fn(X):\n",
        "    return lambda: AdvEnsembleReisz(moment_fn=moment_fn,\n",
        "                                    n_treatments=2,\n",
        "                                    max_abs_value=15,  # originally 26\n",
        "                                    n_iter=40,\n",
        "                                    degree=1)"
      ],
      "metadata": {
        "id": "VncCp0hFuwwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Data"
      ],
      "metadata": {
        "id": "QMhhsgWHux_I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZlhNltVJUcq"
      },
      "outputs": [],
      "source": [
        "# get data\n",
        "np.random.seed(123)\n",
        "df = pd.read_stata('https://github.com/gsbDBI/ExperimentData/raw/master/Charitable/RawData/AER%20merged.dta',\n",
        "                   convert_categoricals=False)\n",
        "df = df.loc[(df['ratio'] == 0) | (df['ratio'] == 1)]\n",
        "df = df.drop(['control', 'ratio', 'ratio2', 'ratio3',\n",
        "            'size', 'size25', 'size50', 'size100', 'sizeno',\n",
        "            'ask', 'askd1', 'askd2', 'askd3', 'ask1', 'ask2', 'ask3',\n",
        "            'amountchange', 'state50one', 'blue0'], axis=1)\n",
        "# state50one just tags one (arbitrary?) observation for each state\n",
        "# blue0 and red0 and perfectly collinear (when all variables are nonmissing); bluecty and redcty are not\n",
        "df = df.dropna()\n",
        "y_amount = df['amount'].values\n",
        "y_gave = df['gave'].values\n",
        "X = df[['treatment', 'red0',\n",
        "        'hpa', 'year5', 'dormant', 'nonlit', 'cases', 'redcty', 'bluecty',\n",
        "        'pwhite', 'pblack', 'page18_39', 'ave_hh_sz', 'median_hhincome', 'powner', 'psch_atlstba', 'pop_propurban']].values"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# synthetic\n",
        "synthetic = False\n",
        "\n",
        "if synthetic:\n",
        "\n",
        "    def true_fn(X):\n",
        "        return X[:, 0] + X[:, 0] * X[:, 1] + X[:, 1] + .1 * X[:, 2]\n",
        "\n",
        "    true = np.mean(moment_fn(X, true_fn))\n",
        "    scale = np.std(true_fn(X)) / 2\n",
        "    print(true, scale)\n",
        "    y_amount = true_fn(X) + np.random.normal(0, scale, size=(X.shape[0],))"
      ],
      "metadata": {
        "id": "sy0mHxD33XDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scaling, Shuffling, and (for Two Treatments) Propensity-Score Filtering"
      ],
      "metadata": {
        "id": "PST14I_7xNu4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(123)\n",
        "\n",
        "# scale non-binary variables\n",
        "y_amount = y_amount.astype(np.double)\n",
        "y_gave = y_gave.astype(np.double)\n",
        "X = X.astype(np.double)\n",
        "idx_nonbi = [i for i in range(2, X.shape[1]) if type_of_target(X[:, i]) != 'binary']  # indices of non-binary variables (first and second columns should be binary)\n",
        "X[:, idx_nonbi] = StandardScaler().fit_transform(X[:, idx_nonbi])\n",
        "y_scale = np.std(y_amount)\n",
        "y_amount = y_amount / y_scale\n",
        "\n",
        "# shuffle data to get random partitions for cross-validation (doing it once at the start is more efficient than repeatedly doing it; the cross-validation here and elsewhere assumes that the data is shuffled)\n",
        "inds = np.arange(X.shape[0])\n",
        "np.random.shuffle(inds)\n",
        "X, y_amount, y_gave = X[inds].copy(), y_amount[inds].copy(), y_gave[inds].copy()\n",
        "\n",
        "# drop extreme party and \"treatment\" (offered-match) propensities\n",
        "clf_party = LogisticRegressionCV(cv=5, max_iter=10000, random_state=123).fit(X[:, 2:], X[:, 1])\n",
        "clf_treat = LogisticRegressionCV(cv=5, max_iter=10000, random_state=123).fit(X[:, 1:], X[:, 0])\n",
        "prop_party = clf_party.predict_proba(X[:, 2:])[:, 1]\n",
        "prop_treat = clf_treat.predict_proba(X[:, 1:])[:, 1]\n",
        "filt = (prop_party <= .9) & (prop_party >= .1) & (prop_treat <= .9) & (prop_treat >= .1)\n",
        "print(X.shape[0], np.sum(filt))\n",
        "X, y_amount, y_gave = X[filt], y_amount[filt], y_gave[filt]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZN52GTDxNPD",
        "outputId": "78ebb449-778e-4f05-fcd7-d23bd44eae5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25859 21712\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Estimation (AER)\n",
        "*The analysis here uses the same regressors as Table 6, Column 9 of Karlan and List (2007). The purpose is, for the purposes of comparison, to get analogous results for the \"two treatment\" case where both offered-match and party-of-state are counterfactually varied.*"
      ],
      "metadata": {
        "id": "KySvif6NMqJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is to replicate set of the regressors using in Table 6, Column 9 in the AER paper\n",
        "def process_regressors(X_old):\n",
        "    \"\"\"Interacts the controls and then adds a constant\"\"\"\n",
        "    return sm.add_constant(np.append(X_old, X_old[:, [0]] * X_old[:, 1:], axis = 1), has_constant = 'add')\n",
        "\n",
        "def moment_fn_withprocessing(x, test_fn):\n",
        "    n_obs = x.shape[0]\n",
        "    t11 = process_regressors(np.hstack([np.ones((n_obs, 2)), x[:, 2:]]))\n",
        "    t01 = process_regressors(np.hstack([np.zeros((n_obs, 1)), np.ones((n_obs, 1)), x[:, 2:]]))\n",
        "    t10 = process_regressors(np.hstack([np.ones((n_obs, 1)), np.zeros((n_obs, 1)), x[:, 2:]]))\n",
        "    t00 = process_regressors(np.hstack([np.zeros((n_obs, 2)), x[:, 2:]]))\n",
        "    return test_fn(t11) - test_fn(t01) - test_fn(t10) + test_fn(t00)"
      ],
      "metadata": {
        "id": "WXLDHcuxHoYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = 0.05\n",
        "n_resamples = 1000\n",
        "\n",
        "# Get data\n",
        "vars = ['treatment', 'red0', 'pwhite', 'pblack',\n",
        "        'page18_39', 'ave_hh_sz', 'median_hhincome',\n",
        "        'powner', 'psch_atlstba', 'pop_propurban']\n",
        "X_AER = df[vars][filt]\n",
        "X_AER = X_AER.values\n",
        "\n",
        "# Calculate critical value\n",
        "critval = sps.norm(loc=0, scale=1).ppf(1-alpha/2)\n",
        "\n",
        "# OLS\n",
        "ols_model = sm.OLS(y_amount*y_scale, process_regressors(X_AER)).fit() # unscale y\n",
        "p = ols_model.params[11]\n",
        "s = ols_model.bse[11]\n",
        "l = p - critval * s\n",
        "u = p + critval * s\n",
        "ols_results = {'point': p, 'stderr': s,\n",
        "               'lower': l, 'upper': u}\n",
        "\n",
        "# Probit (with bootstrapped standard errors)\n",
        "def probit_results_fn(y, X):\n",
        "  probit_model = sm.Probit(y, process_regressors(X)).fit()\n",
        "  moment_pred = moment_fn_withprocessing(X, probit_model.predict)\n",
        "  return mean_ci(moment_pred, confidence = 1-alpha)\n",
        "\n",
        "p, s, l, u = probit_results_fn(y_gave, X_AER)\n",
        "\n",
        "p_boot_list = []\n",
        "for i in range(n_resamples):\n",
        "  idx = np.random.choice(np.arange(len(y_gave)), len(y_gave), replace = True)\n",
        "  X_boot = X_AER[idx, :]\n",
        "  y_boot = y_gave[idx]\n",
        "  p_boot, s_boot, l_boot, u_boot = probit_results_fn(y_boot, X_boot)\n",
        "  p_boot_list.append(p_boot)\n",
        "\n",
        "s = statistics.stdev(p_boot_list)\n",
        "l = p - critval * s\n",
        "u = p + critval * s\n",
        "\n",
        "probit_results = {'point': p, 'stderr': s,\n",
        "                  'lower': l, 'upper': u}"
      ],
      "metadata": {
        "id": "2YYcI2XLHuXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ols_results)\n",
        "print(probit_results)"
      ],
      "metadata": {
        "id": "KKiDnBWsK4JV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZiafKL6JUcq"
      },
      "source": [
        "### Estimation (Riesz)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def do_analysis_charitable(y, n_splits):\n",
        "    res = {}\n",
        "    for name, get_reisz_fn in [\n",
        "                                ('plugin_lg', get_lg_plugin_fn),\n",
        "                                ('plugin_rf', get_rf_plugin_fn),\n",
        "                                ('splin', get_splin_fn),\n",
        "                                # ('advrkhs', get_advkernel_fn),\n",
        "                                ('nys_advrkhs_1000', get_advnyskernel_fn_1000),\n",
        "                                ('nys_advrkhs', get_advnyskernel_fn),\n",
        "                                ('advrf', get_rf_fn),\n",
        "                                ('advnnet', get_agmm_fn),\n",
        "                                ]:\n",
        "        est = DebiasedMoment(moment_fn=moment_fn,\n",
        "                                get_reisz_fn=get_reisz_fn,\n",
        "                                get_reg_fn=get_reg_fn, n_splits=n_splits)\n",
        "        est.fit(X, y)\n",
        "        p, s, l, u = est.avg_moment()\n",
        "        res[name] = {'point': p * y_scale, 'stderr': s * y_scale,\n",
        "                                'lower': l * y_scale, 'upper': u * y_scale}\n",
        "\n",
        "    res = pd.DataFrame(res).transpose()\n",
        "    res"
      ],
      "metadata": {
        "id": "Ii7KM-M943Tp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqeUnOlNJUcq"
      },
      "outputs": [],
      "source": [
        "do_analysis_charitable(y = y_amount, n_splits = 1)\n",
        "joblib.dump(res, f'charitable_amount_ns1.joblib')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "do_analysis_charitable(y = y_amount, n_splits = 5)\n",
        "joblib.dump(res, f'charitable_amount_ns5.joblib')"
      ],
      "metadata": {
        "id": "yeTOM3_5FI-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "do_analysis_charitable(y = y_gave, n_splits = 1)\n",
        "joblib.dump(res, f'charitable_gave_ns1.joblib')"
      ],
      "metadata": {
        "id": "XWlvgn-q8k-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "do_analysis_charitable(y = y_gave, n_splits = 5)\n",
        "joblib.dump(res, f'charitable_gave_ns5.joblib')"
      ],
      "metadata": {
        "id": "a2Bk788r8k0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Synthetic Data Results\n",
        "*This runs the analysis on synthetic data, corresponding to Tables 1-6 in the paper. DGP 0 is the \"simple design\", DGP 3 is the \"nonlinear design\", and DGP 1 is the \"high-dimensional design\".*\n",
        "\n"
      ],
      "metadata": {
        "id": "5uEcYcUt5Z-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def do_analysis_charitable(target_dir, dgp, n_samples_list, sample_its, n_jobs, gcv):\n",
        "    pluginlg_experiments(n_samples_list, dgp=dgp, target_dir=target_dir, sample_its=sample_its, n_jobs=n_jobs, gcv_reg=gcv)\n",
        "    pluginrf_experiments(n_samples_list, dgp=dgp, target_dir=target_dir, sample_its=sample_its, n_jobs=n_jobs, gcv_reg=gcv)\n",
        "    splin_experiments(n_samples_list, dgp=dgp, target_dir=target_dir, sample_its=sample_its, n_jobs=n_jobs, gcv_reg=gcv)\n",
        "    nystrom_advkernel_experiments_1000(n_samples_list, dgp=dgp, target_dir=target_dir, sample_its=sample_its, n_jobs=n_jobs, gcv_reg=gcv)\n",
        "    nystrom_advkernel_experiments(n_samples_list, dgp=dgp, target_dir=target_dir, sample_its=sample_its, n_jobs=n_jobs, gcv_reg=gcv)\n",
        "    rf_experiments(n_samples_list, dgp=dgp, target_dir=target_dir, sample_its=sample_its, n_jobs=n_jobs, gcv_reg=gcv)\n",
        "    nnet_experiments(n_samples_list, dgp=dgp, target_dir=target_dir, sample_its=sample_its, n_jobs=n_jobs, gcv_reg=gcv)\n",
        "\n",
        "    res = [pluginlg_postprocess(n_samples_list, dgp=dgp, target_dir=target_dir, sample_its=sample_its),\n",
        "           pluginrf_postprocess(n_samples_list, dgp=dgp, target_dir=target_dir, sample_its=sample_its),\n",
        "           splin_postprocess(n_samples_list, dgp=dgp, target_dir=target_dir, sample_its=sample_its),\n",
        "           nystrom_advkernel_postprocess_1000(n_samples_list, dgp=dgp, target_dir=target_dir, sample_its=sample_its),\n",
        "           nystrom_advkernel_postprocess(n_samples_list, dgp=dgp, target_dir=target_dir, sample_its=sample_its),\n",
        "           rf_postprocess(n_samples_list, dgp=dgp, target_dir=target_dir, sample_its=sample_its),\n",
        "           nnet_postprocess(n_samples_list, dgp=dgp, target_dir=target_dir, sample_its=sample_its)]"
      ],
      "metadata": {
        "id": "4KwMBMn4i_E-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "do_analysis_charitable(target_dir = '', dgp = 0, n_samples_list = [100, 200, 500, 1000, 2000], sample_its = 100, n_jobs = -1, gcv = True)\n",
        "joblib.dump(res, f'synthetic_dgp0.joblib')"
      ],
      "metadata": {
        "id": "N22bLt6g5cP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "do_analysis_charitable(target_dir = '', dgp = 1, n_samples_list = [1000], sample_its = 100, n_jobs = -1, gcv = True)\n",
        "joblib.dump(res, f'synthetic_dgp1.joblib')"
      ],
      "metadata": {
        "id": "bxNRk6qvLv9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "do_analysis_charitable_all(target_dir = '', dgp = 3, n_samples_list = [100], sample_its = 100, n_jobs = -1, gcv = True)\n",
        "joblib.dump(res, f'synthetic_dgp3.joblib')"
      ],
      "metadata": {
        "id": "_ohZA4hwLwNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXYnVnM0JUcr"
      },
      "source": [
        "# 401(k) Data Results (Not Included in Paper)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Moment Function (One Treatment)"
      ],
      "metadata": {
        "id": "loMcCv1lBMCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# E[E[Y|D=1, X] – E[Y|D=0, X]]\n",
        "# D is the first column and X is the remaining columns.\n",
        "def moment_fn(x, test_fn):\n",
        "    n_obs = x.shape[0]\n",
        "    if torch.is_tensor(x):\n",
        "        with torch.no_grad():\n",
        "            t1 = torch.cat([torch.ones((n_obs, 1)).to(device), x[:, 1:]], dim=1)\n",
        "            t0 = torch.cat([torch.zeros((n_obs, 1)).to(device), x[:, 1:]], dim=1)\n",
        "    else:\n",
        "        t1 = np.hstack([np.ones((n_obs, 1)), x[:, 1:]])\n",
        "        t0 = np.hstack([np.zeros((n_obs, 1)), x[:, 1:]])\n",
        "    return test_fn(t1) - test_fn(t0)"
      ],
      "metadata": {
        "id": "_SUGcYHZt3P3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adversarial Riesz Estimators (One Treatment)"
      ],
      "metadata": {
        "id": "g6WXW_m2BOf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  def get_lg_plugin_fn(X):\n",
        "      clf = LogisticRegressionCV(cv=3, max_iter=10000, random_state=123)\n",
        "      C_ = clf.fit(X[:, 1:], X[:, 0]).C_[0]\n",
        "      model_t_treat = LogisticRegression(C=C_, max_iter=10000, random_state=123)\n",
        "      return lambda: PluginRR(model_t=model_t_treat,\n",
        "                              min_propensity=1e-6)\n",
        "\n",
        "  def get_rf_plugin_fn(X):\n",
        "      gcv = GridSearchCV(RandomForestClassifier(bootstrap=True, random_state=123),\n",
        "                        param_grid={'max_depth': [3, None],\n",
        "                                    'min_samples_leaf': [10, 50]},\n",
        "                        scoring='r2',\n",
        "                        cv=5)\n",
        "      best_model_treat = clone(clone(gcv).fit(X[:, 1:], X[:, 0]).best_estimator_)\n",
        "      return lambda: PluginRR(model_t=best_model_treat,\n",
        "                              min_propensity=1e-6)\n",
        "\n",
        "  def get_rf_fn(X):\n",
        "      return lambda: AdvEnsembleReisz(moment_fn=moment_fn,\n",
        "                                      n_treatments=1,\n",
        "                                      max_abs_value=15,\n",
        "                                      n_iter=40,\n",
        "                                      degree=1)"
      ],
      "metadata": {
        "id": "JSCkY4QqJNmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load, process, and perform estimation on datasets\n",
        "*The observations of the 401(k) data are partitioned into quintile groups of household income. `q = 0` refers to the whole, unsubsetted data. In order to help satify the common support assumption, the data are trimmed in the sense that untreated observations with propensity scores (estimated according to a multinomial logistic model) outside of the hull of the propensity scores of the treated observations are dropped.*"
      ],
      "metadata": {
        "id": "FXV8FwUlx0aN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_splits = 1\n",
        "res = {}\n",
        "\n",
        "for q in np.arange(0, 6):\n",
        "    res[f'q={q}'] = {}\n",
        "    print(f'Quintile={q}')\n",
        "    df = pd.read_csv(f'401k/quintile{q}_trimmed.csv', index_col=0)\n",
        "    y = df['Y'].values\n",
        "    X = df[['D'] + [f'X{i}' for i in np.arange(1, 10)]].values\n",
        "\n",
        "    # scale non-binary variables\n",
        "    y_amount = y_amount.astype(np.double)\n",
        "    y_gave = y_gave.astype(np.double)\n",
        "    X = X.astype(np.double)\n",
        "    idx_nonbi = [i for i in range(2, X.shape[1]) if type_of_target(X[:, i]) != 'binary']  # indices of non-binary variables (first and second columns should be binary)\n",
        "    X[:, idx_nonbi] = StandardScaler().fit_transform(X[:, idx_nonbi])\n",
        "    y_scale = np.std(y_amount)\n",
        "    y_amount = y_amount / y_scale\n",
        "\n",
        "    # shuffle data (doing it once at the start is more efficient than repeatedly doing it; the cross-validation here and elsewhere assumes that the data is shuffled)\n",
        "    inds = np.arange(X.shape[0])\n",
        "    np.random.seed(123)\n",
        "    np.random.shuffle(inds)\n",
        "    X, y = X[inds].copy(), y[inds].copy()\n",
        "\n",
        "    # filter extreme propensities\n",
        "    clf = LogisticRegressionCV(cv=5, max_iter=10000, random_state=123).fit(X[:, 1:], X[:, 0])\n",
        "    prop = clf.predict_proba(X[:, 1:])\n",
        "    filt = (prop[:, 1] <= .9) & (prop[:, 1] >= .1)\n",
        "    print(X.shape[0], np.sum(filt))\n",
        "    X, y = X[filt], y[filt]\n",
        "\n",
        "    for name, get_reisz_fn in [\n",
        "                                ('plugin_lg', get_lg_plugin_fn),\n",
        "                                ('plugin_rf', get_rf_plugin_fn),\n",
        "                                ('splin', get_splin_fn),\n",
        "                                # ('advrkhs', get_advkernel_fn),\n",
        "                                ('nys_advrkhs_1000', get_advnyskernel_fn_1000),\n",
        "                                ('nys_advrkhs', get_advnyskernel_fn),\n",
        "                                ('advrf', get_rf_fn),\n",
        "                                ('advnnet', get_agmm_fn)\n",
        "                                ]:\n",
        "        est = DebiasedMoment(moment_fn=moment_fn,\n",
        "                             get_reisz_fn=get_reisz_fn,\n",
        "                             get_reg_fn=get_reg_fn, n_splits=n_splits)\n",
        "        est.fit(X, y)\n",
        "        p, s, l, u = est.avg_moment()\n",
        "        res[f'q={q}'][name] = {'point': p * y_scale, 'stderr': s * y_scale,\n",
        "                               'lower': l * y_scale, 'upper': u * y_scale}\n",
        "        p, s, l, u = est.avg_moment(tmle=True)\n",
        "        res[f'q={q}'][f'{name}_tmle'] = {'point': p * y_scale, 'stderr': s * y_scale,\n",
        "                                         'lower': l * y_scale, 'upper': u * y_scale}\n",
        "\n",
        "    print(res[f'q={q}'])\n",
        "    res[f'q={q}'] = pd.DataFrame(res[f'q={q}']).transpose()\n",
        "res = pd.concat(res)\n",
        "\n",
        "joblib.dump(res, f'401k_ns{n_splits}.joblib')"
      ],
      "metadata": {
        "id": "8B3e-58JyFRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_splits = 5\n",
        "res = {}\n",
        "\n",
        "for q in np.arange(0, 6):\n",
        "    res[f'q={q}'] = {}\n",
        "    print(f'Quintile={q}')\n",
        "    df = pd.read_csv(f'401k/quintile{q}_trimmed.csv', index_col=0)\n",
        "    y = df['Y'].values\n",
        "    X = df[['D'] + [f'X{i}' for i in np.arange(1, 10)]].values\n",
        "\n",
        "    # scale non-binary variables\n",
        "    y_amount = y_amount.astype(np.double)\n",
        "    y_gave = y_gave.astype(np.double)\n",
        "    X = X.astype(np.double)\n",
        "    idx_nonbi = [i for i in range(2, X.shape[1]) if type_of_target(X[:, i]) != 'binary']  # indices of non-binary variables (first and second columns should be binary)\n",
        "    X[:, idx_nonbi] = StandardScaler().fit_transform(X[:, idx_nonbi])\n",
        "    y_scale = np.std(y_amount)\n",
        "    y_amount = y_amount / y_scale\n",
        "\n",
        "    # shuffle data (doing it once at the start is more efficient than repeatedly doing it; the cross-validation here and elsewhere assumes that the data is shuffled)\n",
        "    inds = np.arange(X.shape[0])\n",
        "    np.random.seed(123)\n",
        "    np.random.shuffle(inds)\n",
        "    X, y = X[inds].copy(), y[inds].copy()\n",
        "\n",
        "    # filter extreme propensities\n",
        "    clf = LogisticRegressionCV(cv=5, max_iter=10000, random_state=123).fit(X[:, 1:], X[:, 0])\n",
        "    prop = clf.predict_proba(X[:, 1:])\n",
        "    filt = (prop[:, 1] <= .9) & (prop[:, 1] >= .1)\n",
        "    print(X.shape[0], np.sum(filt))\n",
        "    X, y = X[filt], y[filt]\n",
        "\n",
        "    for name, get_reisz_fn in [\n",
        "                                ('plugin_lg', get_lg_plugin_fn),\n",
        "                                ('plugin_rf', get_rf_plugin_fn),\n",
        "                                ('splin', get_splin_fn),\n",
        "                                # ('advrkhs', get_advkernel_fn),\n",
        "                                ('nys_advrkhs_1000', get_advnyskernel_fn_1000),\n",
        "                                ('nys_advrkhs', get_advnyskernel_fn),\n",
        "                                ('advrf', get_rf_fn),\n",
        "                                ('advnnet', get_agmm_fn)\n",
        "                                ]:\n",
        "        est = DebiasedMoment(moment_fn=moment_fn,\n",
        "                             get_reisz_fn=get_reisz_fn,\n",
        "                             get_reg_fn=get_reg_fn, n_splits=n_splits)\n",
        "        est.fit(X, y)\n",
        "        p, s, l, u = est.avg_moment()\n",
        "        res[f'q={q}'][name] = {'point': p * y_scale, 'stderr': s * y_scale,\n",
        "                               'lower': l * y_scale, 'upper': u * y_scale}\n",
        "        p, s, l, u = est.avg_moment(tmle=True)\n",
        "        res[f'q={q}'][f'{name}_tmle'] = {'point': p * y_scale, 'stderr': s * y_scale,\n",
        "                                         'lower': l * y_scale, 'upper': u * y_scale}\n",
        "\n",
        "    print(res[f'q={q}'])\n",
        "    res[f'q={q}'] = pd.DataFrame(res[f'q={q}']).transpose()\n",
        "res = pd.concat(res)\n",
        "\n",
        "joblib.dump(res, f'401k_ns{n_splits}.joblib')"
      ],
      "metadata": {
        "id": "aWfR4A3hyTBz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}